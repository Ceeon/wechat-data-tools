#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯æ—¥æ‰¹é‡è·å–è„šæœ¬
åŠŸèƒ½: è·å–å½“å¤©å‘å¸ƒçš„æ‰€æœ‰æ–‡ç« 
"""

import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

import feedparser
import requests
from bs4 import BeautifulSoup
import html2text
import yaml
import re
import time
import hashlib
import json


# é…ç½®æ–‡ä»¶è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
CONFIG_FILE = PROJECT_ROOT / "config" / "config.yaml"
SUBSCRIPTIONS_FILE = PROJECT_ROOT / "config" / "subscriptions.csv"


def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def load_subscriptions():
    """åŠ è½½è®¢é˜…åˆ—è¡¨"""
    subscriptions = []
    with open(SUBSCRIPTIONS_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            if line.startswith('name,'):
                continue

            parts = line.split(',')
            if len(parts) >= 3:
                subscriptions.append({
                    'name': parts[0],
                    'biz': parts[1],
                    'rss_url': parts[2],
                    'category': parts[3] if len(parts) > 3 else ''
                })
    return subscriptions


def is_today(pub_date_str):
    """
    åˆ¤æ–­æ–‡ç« æ˜¯å¦æ˜¯ä»Šå¤©å‘å¸ƒçš„

    Args:
        pub_date_str: å‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²(RSSæ ¼å¼)

    Returns:
        bool: æ˜¯ä»Šå¤©è¿”å›True
    """
    try:
        from dateutil import parser
        pub_date = parser.parse(pub_date_str)

        # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº
        today = datetime.now().date()
        pub_date_local = pub_date.date()

        return pub_date_local == today
    except Exception as e:
        print(f"  âš ï¸  æ—¶é—´è§£æå¤±è´¥: {e}")
        return False


def is_yesterday(pub_date_str):
    """
    åˆ¤æ–­æ–‡ç« æ˜¯å¦æ˜¯æ˜¨å¤©å‘å¸ƒçš„

    Args:
        pub_date_str: å‘å¸ƒæ—¶é—´å­—ç¬¦ä¸²(RSSæ ¼å¼)

    Returns:
        bool: æ˜¯æ˜¨å¤©è¿”å›True
    """
    try:
        from dateutil import parser
        pub_date = parser.parse(pub_date_str)

        # è½¬æ¢ä¸ºæœ¬åœ°æ—¶åŒº
        yesterday = (datetime.now() - timedelta(days=1)).date()
        pub_date_local = pub_date.date()

        return pub_date_local == yesterday
    except Exception as e:
        print(f"  âš ï¸  æ—¶é—´è§£æå¤±è´¥: {e}")
        return False


def get_article_id(url):
    """ä»æ–‡ç« URLç”Ÿæˆå”¯ä¸€ID"""
    return hashlib.md5(url.encode()).hexdigest()[:16]


def check_article_exists(article_id, articles_dir):
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨"""
    for item in Path(articles_dir).glob(f"*_{article_id}_*"):
        if item.is_dir():
            return True
    return False


def download_article_html(url, timeout=30):
    """ä¸‹è½½æ–‡ç« HTMLå†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"  âŒ ä¸‹è½½æ–‡ç« å¤±è´¥: {e}")
        return None


def extract_article_content(html):
    """ä»HTMLä¸­æå–æ–‡ç« æ­£æ–‡"""
    soup = BeautifulSoup(html, 'lxml')

    title = soup.find('h1', class_='rich_media_title')
    title = title.get_text().strip() if title else "æ— æ ‡é¢˜"

    author = soup.find('a', class_='rich_media_meta_link')
    author = author.get_text().strip() if author else "æœªçŸ¥ä½œè€…"

    content = soup.find('div', class_='rich_media_content')
    content_html = str(content) if content else ""

    return {
        'title': title,
        'author': author,
        'content_html': content_html
    }


def html_to_markdown(html):
    """å°†HTMLè½¬æ¢ä¸ºMarkdown"""
    h = html2text.HTML2Text()
    h.ignore_links = False
    h.ignore_images = False
    h.ignore_emphasis = False
    h.body_width = 0

    markdown = h.handle(html)
    return markdown


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶å,ç§»é™¤éæ³•å­—ç¬¦"""
    filename = re.sub(r'[<>:"/\\|?*]', '', filename)
    if len(filename) > 50:
        filename = filename[:50]
    return filename


def save_article(article_data, articles_dir):
    """ä¿å­˜æ–‡ç« ä¸ºMarkdownæ–‡ä»¶"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    article_id = article_data['id']
    title = sanitize_filename(article_data['title'])
    folder_name = f"{timestamp}_{article_id}_{title}"

    article_folder = Path(articles_dir) / folder_name
    article_folder.mkdir(parents=True, exist_ok=True)

    md_content = f"""# {article_data['title']}

**ä½œè€…**: {article_data['author']}
**å‘å¸ƒæ—¶é—´**: {article_data['publish_time']}
**åŸæ–‡é“¾æ¥**: {article_data['url']}
**å…¬ä¼—å·**: {article_data['account_name']}
**åˆ†ç±»**: {article_data['category']}
**é‡‡é›†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

{article_data['content_md']}
"""

    md_file = article_folder / "article.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    # ä¿å­˜ metadata.json
    metadata = {
        'title': article_data['title'],
        'author': article_data['author'],
        'publish_time': article_data['publish_time'],
        'url': article_data['url'],
        'account_name': article_data['account_name'],
        'category': article_data['category'],
        'collected_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }

    metadata_file = article_folder / "metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"  âœ… å·²ä¿å­˜: {article_folder.name}")
    return str(article_folder)


def fetch_today_articles():
    """è·å–ä»Šå¤©çš„æ–‡ç« """
    import argparse

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='é‡‡é›†å…¬ä¼—å·æ–‡ç« ')
    parser.add_argument('--mode', choices=['today', 'yesterday', 'all', 'recent'], default='yesterday',
                       help='é‡‡é›†æ¨¡å¼: yesterday=åªé‡‡é›†æ˜¨å¤©(é»˜è®¤), today=åªé‡‡é›†ä»Šå¤©, all=é‡‡é›†æ‰€æœ‰æœªé‡‡é›†çš„, recent=é‡‡é›†æœ€è¿‘Nç¯‡')
    parser.add_argument('--limit', type=int, default=20,
                       help='recentæ¨¡å¼ä¸‹é‡‡é›†çš„æ•°é‡(é»˜è®¤20)')
    args = parser.parse_args()

    print("=" * 60)
    if args.mode == 'today':
        print(f"ğŸ“… æ¯æ—¥æ–‡ç« é‡‡é›† - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
    elif args.mode == 'yesterday':
        yesterday_date = (datetime.now() - timedelta(days=1)).strftime('%Yå¹´%mæœˆ%dæ—¥')
        print(f"ğŸ“… æ¯æ—¥æ–‡ç« é‡‡é›† - {yesterday_date} (æ˜¨å¤©)")
    elif args.mode == 'all':
        print(f"ğŸ“… å…¨é‡æ–‡ç« é‡‡é›† - é‡‡é›†æ‰€æœ‰æœªé‡‡é›†çš„æ–‡ç« ")
    else:
        print(f"ğŸ“… æœ€è¿‘æ–‡ç« é‡‡é›† - é‡‡é›†æœ€è¿‘{args.limit}ç¯‡æœªé‡‡é›†çš„æ–‡ç« ")
    print("=" * 60)

    # åŠ è½½é…ç½®
    config = load_config()
    subscriptions = load_subscriptions()
    articles_dir = PROJECT_ROOT / config['storage']['articles_dir']

    print(f"\nğŸ“‹ åŠ è½½äº† {len(subscriptions)} ä¸ªè®¢é˜…")

    total_found = 0
    total_new = 0

    # éå†æ¯ä¸ªè®¢é˜…
    for sub in subscriptions:
        print(f"\n{'='*60}")
        print(f"ğŸ“¡ å¤„ç†è®¢é˜…: {sub['name']} ({sub['category']})")
        print(f"{'='*60}")

        # è·å–RSS
        try:
            feed = feedparser.parse(sub['rss_url'])
            if not feed or not hasattr(feed, 'entries'):
                print(f"  âŒ RSSæºæ— æ•ˆæˆ–æ— æ–‡ç« ")
                continue

            print(f"  ğŸ“ RSSä¸­å…±æœ‰ {len(feed.entries)} ç¯‡æ–‡ç« ")

            # æ ¹æ®æ¨¡å¼ç­›é€‰æ–‡ç« 
            target_entries = []

            if args.mode == 'today':
                # åªé‡‡é›†ä»Šå¤©çš„æ–‡ç« 
                for entry in feed.entries:
                    if hasattr(entry, 'updated'):
                        if is_today(entry.updated):
                            target_entries.append(entry)
                if not target_entries:
                    print(f"  â­ï¸  ä»Šå¤©æ²¡æœ‰æ–°æ–‡ç« ")
                    continue
                print(f"  âœ¨ ä»Šå¤©å‘å¸ƒäº† {len(target_entries)} ç¯‡æ–‡ç« ")

            elif args.mode == 'yesterday':
                # åªé‡‡é›†æ˜¨å¤©çš„æ–‡ç« 
                for entry in feed.entries:
                    if hasattr(entry, 'updated'):
                        if is_yesterday(entry.updated):
                            target_entries.append(entry)
                if not target_entries:
                    print(f"  â­ï¸  æ˜¨å¤©æ²¡æœ‰æ–°æ–‡ç« ")
                    continue
                print(f"  âœ¨ æ˜¨å¤©å‘å¸ƒäº† {len(target_entries)} ç¯‡æ–‡ç« ")

            elif args.mode == 'all':
                # é‡‡é›†æ‰€æœ‰æœªé‡‡é›†çš„æ–‡ç« 
                target_entries = feed.entries
                print(f"  ğŸ” æ£€æŸ¥æ‰€æœ‰æ–‡ç« ...")

            elif args.mode == 'recent':
                # é‡‡é›†æœ€è¿‘Nç¯‡æœªé‡‡é›†çš„æ–‡ç« 
                target_entries = feed.entries[:args.limit]
                print(f"  ğŸ” æ£€æŸ¥æœ€è¿‘ {len(target_entries)} ç¯‡æ–‡ç« ...")

            total_found += len(target_entries)

            # å¤„ç†ç­›é€‰å‡ºçš„æ–‡ç« 
            for entry in target_entries:
                try:
                    url = entry.link
                    article_id = get_article_id(url)
                    title = entry.title if hasattr(entry, 'title') else "æ— æ ‡é¢˜"

                    print(f"\n  å¤„ç†æ–‡ç« : {title}")
                    print(f"    URL: {url}")

                    # å»é‡æ£€æŸ¥
                    if check_article_exists(article_id, articles_dir):
                        print(f"    â­ï¸  æ–‡ç« å·²å­˜åœ¨,è·³è¿‡")
                        continue

                    # è·å–RSSä¸­çš„å‘å¸ƒæ—¶é—´å¹¶æ ¼å¼åŒ–
                    publish_time = ""
                    if hasattr(entry, 'updated'):
                        try:
                            from dateutil import parser
                            dt = parser.parse(entry.updated)
                            publish_time = dt.strftime('%Y-%m-%d %H:%M:%S')
                        except:
                            publish_time = entry.updated

                    # ä¼˜å…ˆä½¿ç”¨RSS feedä¸­çš„å†…å®¹ï¼ˆWechat2RSSå·²ç»åŒ…å«å®Œæ•´å†…å®¹ï¼‰
                    content_html = ""
                    author = "æœªçŸ¥ä½œè€…"

                    if hasattr(entry, 'content') and entry.content:
                        # RSS feed ä¸­æœ‰å®Œæ•´å†…å®¹
                        content_html = entry.content[0].value
                        print(f"    âœ… ä»RSSè·å–å†…å®¹ ({len(content_html)} å­—ç¬¦)")
                    else:
                        # å¤‡ç”¨æ–¹æ¡ˆï¼šä¸‹è½½HTML
                        print(f"    âš ï¸  RSSæ— å†…å®¹ï¼Œå°è¯•ä¸‹è½½HTML...")
                        html = download_article_html(url, timeout=config['rss']['timeout'])
                        if not html:
                            continue

                        # æå–å†…å®¹
                        content = extract_article_content(html)
                        content_html = content['content_html']
                        author = content['author']

                    # è½¬æ¢ä¸ºMarkdown
                    content_md = html_to_markdown(content_html)

                    # ç»„è£…æ–‡ç« æ•°æ®
                    # ä¼˜å…ˆä½¿ç”¨RSSä¸­çš„æ ‡é¢˜
                    final_title = title if title != "æ— æ ‡é¢˜" else "æ— æ ‡é¢˜"

                    article_data = {
                        'id': article_id,
                        'url': url,
                        'title': final_title,
                        'author': author,
                        'publish_time': publish_time,  # ä½¿ç”¨RSSæ—¶é—´
                        'content_md': content_md,
                        'account_name': sub['name'],
                        'category': sub['category']
                    }

                    # ä¿å­˜æ–‡ç« 
                    save_article(article_data, articles_dir)
                    total_new += 1

                    # å»¶è¿Ÿ
                    time.sleep(1)

                except Exception as e:
                    print(f"    âŒ å¤„ç†å¤±è´¥: {e}")
                    continue

        except Exception as e:
            print(f"  âŒ è®¢é˜…å¤„ç†å¤±è´¥: {e}")
            continue

    print(f"\n{'='*60}")
    print(f"âœ… é‡‡é›†å®Œæˆ!")
    print(f"   æ£€æŸ¥äº†: {total_found} ç¯‡æ–‡ç« ")
    print(f"   æ–°å¢ä¿å­˜: {total_new} ç¯‡æ–‡ç« ")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    fetch_today_articles()
