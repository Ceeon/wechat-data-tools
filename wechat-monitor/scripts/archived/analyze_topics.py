#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€‰é¢˜æ•°æ®åˆ†æè„šæœ¬
åˆ†æå“ªäº›é€‰é¢˜æ–¹å‘æ•°æ®è¡¨ç°å¥½ï¼Œæ‰¾å‡ºå…±åŒç‰¹å¾
"""

import json
import re
from pathlib import Path
from collections import defaultdict
import statistics

PROJECT_ROOT = Path(__file__).parent.parent
ARTICLES_DIR = PROJECT_ROOT / "data" / "articles"


def load_article_data():
    """åŠ è½½æ‰€æœ‰æ–‡ç« æ•°æ®"""
    articles = []

    for article_dir in ARTICLES_DIR.glob("*"):
        if not article_dir.is_dir():
            continue

        metadata_file = article_dir / "metadata.json"
        stats_file = article_dir / "stats_metadata.json"

        if not metadata_file.exists():
            continue

        # è¯»å–å…ƒæ•°æ®
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        # è¯»å–äº’åŠ¨æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        stats = {}
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)

        # åˆå¹¶æ•°æ®
        article = {
            **metadata,
            'read_num': stats.get('read_num', 0),
            'like_num': stats.get('like_num', 0),
            'looking_num': stats.get('looking_num', 0),
            'in_comment_num': stats.get('in_comment_num', 0),
            'engagement_rate': 0  # äº’åŠ¨ç‡
        }

        # è®¡ç®—äº’åŠ¨ç‡ = (ç‚¹èµ + åœ¨çœ‹ + è¯„è®º) / é˜…è¯»æ•°
        if article['read_num'] > 0:
            article['engagement_rate'] = (
                article['like_num'] +
                article['looking_num'] +
                article['in_comment_num']
            ) / article['read_num'] * 100

        articles.append(article)

    return articles


def extract_keywords(title):
    """ä»æ ‡é¢˜ä¸­æå–å…³é”®è¯"""
    # å¸¸è§çš„æŠ€æœ¯/äº§å“å…³é”®è¯
    ai_tools = ['GPT', 'ChatGPT', 'Claude', 'Sora', 'Midjourney', 'Stable Diffusion',
                'Deepseek', 'OpenAI', 'AI', 'RPA', 'n8n', 'workflow', 'Veo']

    tech_terms = ['ç¼–ç¨‹', 'å¼€å‘', 'å·¥ä½œæµ', 'è‡ªåŠ¨åŒ–', 'ç”Ÿå›¾', 'è§†é¢‘', 'æ¨¡å‹',
                  'å¼€æº', 'ä»£ç ', 'Code', 'å‡ºæµ·']

    business_terms = ['èµšé’±', 'å˜ç°', 'å¢é•¿', 'ç­–ç•¥', 'çº¢åˆ©', 'æµé‡', 'ç¾é‡‘',
                      'ç”µå•†', 'äº§å“', 'ç»„ç»‡']

    keywords = {
        'ai_tools': [],
        'tech_terms': [],
        'business_terms': []
    }

    for tool in ai_tools:
        if tool.lower() in title.lower():
            keywords['ai_tools'].append(tool)

    for term in tech_terms:
        if term in title:
            keywords['tech_terms'].append(term)

    for term in business_terms:
        if term in title:
            keywords['business_terms'].append(term)

    return keywords


def analyze_top_performers(articles, top_n=20):
    """åˆ†æè¡¨ç°æœ€å¥½çš„æ–‡ç« """
    # æŒ‰é˜…è¯»æ•°æ’åº
    sorted_by_reads = sorted(articles, key=lambda x: x['read_num'], reverse=True)
    top_reads = sorted_by_reads[:top_n]

    # æŒ‰äº’åŠ¨ç‡æ’åº
    sorted_by_engagement = sorted(articles, key=lambda x: x['engagement_rate'], reverse=True)
    top_engagement = sorted_by_engagement[:top_n]

    return top_reads, top_engagement


def find_common_patterns(articles):
    """æ‰¾å‡ºå…±åŒç‰¹å¾"""
    all_keywords = defaultdict(list)

    for article in articles:
        keywords = extract_keywords(article['title'])

        for key, values in keywords.items():
            all_keywords[key].extend(values)

    # ç»Ÿè®¡å…³é”®è¯é¢‘ç‡
    from collections import Counter

    patterns = {
        'ai_tools': Counter(all_keywords['ai_tools']).most_common(10),
        'tech_terms': Counter(all_keywords['tech_terms']).most_common(10),
        'business_terms': Counter(all_keywords['business_terms']).most_common(10)
    }

    return patterns


def analyze_title_patterns(articles):
    """åˆ†ææ ‡é¢˜æ¨¡å¼"""
    patterns = {
        'æ•°å­—å‹': 0,  # åŒ…å«æ•°å­—
        'å¯¹æ¯”å‹': 0,  # åŒ…å« vsã€å¯¹æ¯”ã€è¶…è¶Š
        'åŠé€€å‹': 0,  # åŠé€€ã€ä¸è¦å­¦ã€è¿‡æ—¶
        'å®æˆ˜å‹': 0,  # å®æˆ˜ã€æ•™ç¨‹ã€æŠ€å·§ã€æ–¹æ³•
        'çˆ†æ–™å‹': 0,  # æ›å…‰ã€æ­ç§˜ã€æ‹†è§£
        'é—®é¢˜å‹': 0,  # åŒ…å«é—®å·
    }

    for article in articles:
        title = article['title']

        if re.search(r'\d+', title):
            patterns['æ•°å­—å‹'] += 1

        if any(word in title for word in ['vs', 'VS', 'å¯¹æ¯”', 'è¶…è¶Š', 'ç¡¬åˆš']):
            patterns['å¯¹æ¯”å‹'] += 1

        if any(word in title for word in ['åŠé€€', 'ä¸è¦å­¦', 'è¿‡æ—¶']):
            patterns['åŠé€€å‹'] += 1

        if any(word in title for word in ['å®æˆ˜', 'æ•™ç¨‹', 'æŠ€å·§', 'æ–¹æ³•', 'ç©æ³•', 'ç­–ç•¥']):
            patterns['å®æˆ˜å‹'] += 1

        if any(word in title for word in ['æ›å…‰', 'æ­ç§˜', 'æ‹†è§£', 'æ‰’']):
            patterns['çˆ†æ–™å‹'] += 1

        if 'ï¼Ÿ' in title or '?' in title:
            patterns['é—®é¢˜å‹'] += 1

    return patterns


def generate_topic_suggestions(patterns):
    """æ ¹æ®åˆ†æç»“æœç”Ÿæˆé€‰é¢˜å»ºè®®"""
    suggestions = []

    # åŸºäºé«˜é¢‘å…³é”®è¯ç»„åˆç”Ÿæˆå»ºè®®
    suggestions.append({
        'title': 'Claude 3.5 Sonnet vs GPT-4ï¼šæˆ‘æµ‹è¯•äº†10ä¸ªç¼–ç¨‹åœºæ™¯ï¼Œç»“æœå¤ªæ„å¤–äº†',
        'reason': 'ç»“åˆçƒ­é—¨AIå·¥å…·å¯¹æ¯” + æ•°å­—åŒ– + å®æµ‹åœºæ™¯'
    })

    suggestions.append({
        'title': 'åŠé€€ï¼šåˆ«å†æ‰‹åŠ¨åšè§†é¢‘äº†ï¼Œè¿™ä¸ªAIå·¥ä½œæµ5åˆ†é’Ÿç”Ÿæˆçˆ†æ¬¾',
        'reason': 'åŠé€€å‹æ ‡é¢˜ + æ•ˆç‡å¯¹æ¯” + å®æˆ˜å·¥å…·'
    })

    suggestions.append({
        'title': 'æ‹†è§£ï¼šè¿™ä¸ªAIå‡ºæµ·äº§å“æœˆå…¥10ä¸‡ç¾é‡‘çš„å®Œæ•´ç­–ç•¥',
        'reason': 'æ‹†è§£å‹ + å…·ä½“æ•°å­— + å•†ä¸šå˜ç°'
    })

    suggestions.append({
        'title': 'OpenAIåˆšå‘å¸ƒçš„è§†é¢‘åŠŸèƒ½ï¼Œæˆ‘æ€»ç»“äº†8ä¸ªè¶…å®ç”¨æŠ€å·§',
        'reason': 'çƒ­ç‚¹è¿½è¸ª + æ•°å­—åŒ– + å®æˆ˜æŠ€å·§'
    })

    suggestions.append({
        'title': 'Cursorè¶…è¶ŠClaudeï¼Ÿå®æµ‹100ä¸ªä»£ç åœºæ™¯ï¼Œå·®è·åœ¨è¿™é‡Œ',
        'reason': 'å·¥å…·å¯¹æ¯” + å¤§é‡æµ‹è¯• + æ‚¬å¿µåˆ¶é€ '
    })

    return suggestions


def print_analysis(articles):
    """æ‰“å°å®Œæ•´åˆ†ææŠ¥å‘Š"""
    print("=" * 80)
    print("ğŸ“Š æ–‡ç« æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()

    # åŸºç¡€ç»Ÿè®¡
    print(f"ğŸ“š æ€»æ–‡ç« æ•°: {len(articles)}")
    print(f"ğŸ“– æœ‰é˜…è¯»æ•°æ®çš„æ–‡ç« : {len([a for a in articles if a['read_num'] > 0])}")
    print()

    # åˆ†æè¡¨ç°æœ€å¥½çš„æ–‡ç« 
    top_reads, top_engagement = analyze_top_performers(articles, top_n=20)

    print("=" * 80)
    print("ğŸ† 1. è¡¨ç°æœ€å¥½çš„é€‰é¢˜æ–¹å‘ï¼ˆæŒ‰é˜…è¯»æ•°ï¼‰")
    print("=" * 80)
    print()

    for i, article in enumerate(top_reads[:10], 1):
        print(f"{i}. ã€{article.get('category', 'æœªåˆ†ç±»')}ã€‘{article['title']}")
        print(f"   é˜…è¯»: {article['read_num']:,} | ç‚¹èµ: {article['like_num']:,} | "
              f"åœ¨çœ‹: {article['looking_num']:,} | äº’åŠ¨ç‡: {article['engagement_rate']:.2f}%")
        print()

    print("=" * 80)
    print("ğŸ’¡ 2. é«˜é˜…è¯»æ–‡ç« çš„å…±åŒç‰¹å¾")
    print("=" * 80)
    print()

    # åˆ†æå…³é”®è¯
    patterns = find_common_patterns(top_reads)

    print("ğŸ¤– é«˜é¢‘ AI å·¥å…·/æŠ€æœ¯:")
    for keyword, count in patterns['ai_tools'][:5]:
        print(f"   â€¢ {keyword}: {count}æ¬¡")
    print()

    print("ğŸ”§ é«˜é¢‘æŠ€æœ¯è¯æ±‡:")
    for keyword, count in patterns['tech_terms'][:5]:
        print(f"   â€¢ {keyword}: {count}æ¬¡")
    print()

    print("ğŸ’° é«˜é¢‘å•†ä¸šè¯æ±‡:")
    for keyword, count in patterns['business_terms'][:5]:
        print(f"   â€¢ {keyword}: {count}æ¬¡")
    print()

    # æ ‡é¢˜æ¨¡å¼åˆ†æ
    title_patterns = analyze_title_patterns(top_reads)
    print("ğŸ“ æ ‡é¢˜æ¨¡å¼åˆ†æ:")
    for pattern, count in sorted(title_patterns.items(), key=lambda x: x[1], reverse=True):
        percentage = count / len(top_reads) * 100
        print(f"   â€¢ {pattern}: {count}ç¯‡ ({percentage:.1f}%)")
    print()

    # ç»Ÿè®¡æ•°æ®ç‰¹å¾
    avg_reads = statistics.mean([a['read_num'] for a in top_reads if a['read_num'] > 0])
    avg_engagement = statistics.mean([a['engagement_rate'] for a in top_reads])

    print("ğŸ“ˆ å¹³å‡æ•°æ®:")
    print(f"   â€¢ å¹³å‡é˜…è¯»æ•°: {avg_reads:,.0f}")
    print(f"   â€¢ å¹³å‡äº’åŠ¨ç‡: {avg_engagement:.2f}%")
    print()

    print("=" * 80)
    print("âœ¨ 3. åŸºäºæ•°æ®çš„é€‰é¢˜å»ºè®®ï¼ˆ5ä¸ªï¼‰")
    print("=" * 80)
    print()

    suggestions = generate_topic_suggestions(patterns)

    for i, suggestion in enumerate(suggestions, 1):
        print(f"{i}. {suggestion['title']}")
        print(f"   ğŸ’¡ æ¨èç†ç”±: {suggestion['reason']}")
        print()

    print("=" * 80)
    print("ğŸ“‹ é€‰é¢˜ç­–ç•¥æ€»ç»“")
    print("=" * 80)
    print()
    print("âœ… æ ¸å¿ƒè¦ç´ :")
    print("   1. çƒ­é—¨AIå·¥å…·å¯¹æ¯”ï¼ˆSora, Claude, GPTç­‰ï¼‰")
    print("   2. æ•°å­—åŒ–æ ‡é¢˜ï¼ˆå…·ä½“åœºæ™¯æ•°ã€æµ‹è¯•æ•°ï¼‰")
    print("   3. å®æˆ˜å¯¼å‘ï¼ˆæŠ€å·§ã€æ–¹æ³•ã€ç­–ç•¥ï¼‰")
    print("   4. å•†ä¸šä»·å€¼ï¼ˆå‡ºæµ·ã€èµšé’±ã€å˜ç°ï¼‰")
    print("   5. æ ‡é¢˜æŠ€å·§ï¼ˆåŠé€€å‹ã€æ‹†è§£å‹ã€å¯¹æ¯”å‹ï¼‰")
    print()
    print("âš ï¸  é¿å…:")
    print("   â€¢ çº¯ç†è®ºã€æ— å®æˆ˜")
    print("   â€¢ æ ‡é¢˜å¹³æ·¡ã€æ— å¸å¼•åŠ›")
    print("   â€¢ ç¼ºå°‘æ•°å­—ã€æ¡ˆä¾‹æ”¯æ’‘")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨åŠ è½½æ–‡ç« æ•°æ®...")
    articles = load_article_data()

    if not articles:
        print("âŒ æœªæ‰¾åˆ°æ–‡ç« æ•°æ®")
        return

    # åªåˆ†ææœ‰é˜…è¯»æ•°æ®çš„æ–‡ç« 
    articles_with_stats = [a for a in articles if a['read_num'] > 0]

    if not articles_with_stats:
        print("âŒ æœªæ‰¾åˆ°æœ‰äº’åŠ¨æ•°æ®çš„æ–‡ç« ")
        return

    print(f"âœ… å·²åŠ è½½ {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(articles_with_stats)} ç¯‡æœ‰äº’åŠ¨æ•°æ®")
    print()

    # æ‰“å°åˆ†ææŠ¥å‘Š
    print_analysis(articles_with_stats)


if __name__ == "__main__":
    main()
