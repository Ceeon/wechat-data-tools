#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶é—´ç»´åº¦æ•°æ®åˆ†æè„šæœ¬
åˆ†ææœ€è¿‘30å¤©çš„å‘å¸ƒè§„å¾‹ã€æœ€ä½³å‘å¸ƒæ—¶é—´ã€æ ‡é¢˜ç±»å‹æ•ˆæœã€å†…å®¹é•¿åº¦å…³ç³»
"""

import json
import re
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import statistics

PROJECT_ROOT = Path(__file__).parent.parent
ARTICLES_DIR = PROJECT_ROOT / "data" / "articles"


def load_article_data_with_time():
    """åŠ è½½æ‰€æœ‰æ–‡ç« æ•°æ®ï¼ŒåŒ…å«æ—¶é—´ä¿¡æ¯"""
    articles = []

    for article_dir in ARTICLES_DIR.glob("*"):
        if not article_dir.is_dir():
            continue

        # è§£ææ–‡ä»¶å¤¹åç§°: 20251018_033536_id_title
        parts = article_dir.name.split('_')
        if len(parts) < 4:
            continue

        date_str = parts[0]  # 20251018
        time_str = parts[1]  # 033536

        metadata_file = article_dir / "metadata.json"
        stats_file = article_dir / "stats_metadata.json"
        article_file = article_dir / "article.md"

        if not metadata_file.exists():
            continue

        # è¯»å–å…ƒæ•°æ®
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        # è¯»å–äº’åŠ¨æ•°æ®
        stats = {}
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)

        # è¯»å–æ–‡ç« å†…å®¹ä»¥è®¡ç®—é•¿åº¦
        content_length = 0
        if article_file.exists():
            with open(article_file, 'r', encoding='utf-8') as f:
                content = f.read()
                content_length = len(content)

        # è§£æå‘å¸ƒæ—¶é—´
        try:
            publish_time = datetime.strptime(f"{date_str} {time_str}", "%Y%m%d %H%M%S")
        except:
            continue

        # åˆå¹¶æ•°æ®
        article = {
            **metadata,
            'publish_time': publish_time,
            'publish_date': date_str,
            'publish_hour': int(time_str[:2]),
            'weekday': publish_time.weekday(),  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
            'weekday_name': ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][publish_time.weekday()],
            'read_num': stats.get('read_num', 0),
            'like_num': stats.get('like_num', 0),
            'looking_num': stats.get('looking_num', 0),
            'in_comment_num': stats.get('in_comment_num', 0),
            'content_length': content_length,
            'engagement_rate': 0
        }

        # è®¡ç®—äº’åŠ¨ç‡
        if article['read_num'] > 0:
            article['engagement_rate'] = (
                article['like_num'] +
                article['looking_num'] +
                article['in_comment_num']
            ) / article['read_num'] * 100

        articles.append(article)

    return articles


def filter_recent_days(articles, days=30):
    """ç­›é€‰æœ€è¿‘Nå¤©çš„æ–‡ç« """
    cutoff_date = datetime.now() - timedelta(days=days)
    return [a for a in articles if a['publish_time'] >= cutoff_date]


def analyze_update_frequency(articles):
    """åˆ†ææ›´æ–°é¢‘ç‡"""
    if not articles:
        return {}

    # æŒ‰æ—¥æœŸåˆ†ç»„
    by_date = defaultdict(list)
    for article in articles:
        by_date[article['publish_date']].append(article)

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    days_with_content = len(by_date)
    total_articles = len(articles)

    # è®¡ç®—æ—¥æœŸèŒƒå›´
    dates = sorted([datetime.strptime(d, "%Y%m%d") for d in by_date.keys()])
    if len(dates) >= 2:
        date_range = (dates[-1] - dates[0]).days + 1
    else:
        date_range = 1

    # æ¯å¤©æ–‡ç« æ•°åˆ†å¸ƒ
    articles_per_day = [len(articles) for articles in by_date.values()]

    return {
        'total_days': date_range,
        'days_with_content': days_with_content,
        'total_articles': total_articles,
        'avg_per_day': total_articles / date_range if date_range > 0 else 0,
        'avg_per_active_day': statistics.mean(articles_per_day) if articles_per_day else 0,
        'max_per_day': max(articles_per_day) if articles_per_day else 0,
        'publish_rate': days_with_content / date_range * 100 if date_range > 0 else 0,
        'by_date': dict(by_date)
    }


def analyze_best_publish_time(articles):
    """åˆ†ææœ€ä½³å‘å¸ƒæ—¶é—´"""
    # æŒ‰æ˜ŸæœŸå‡ åˆ†ç»„
    by_weekday = defaultdict(list)
    for article in articles:
        if article['read_num'] > 0:
            by_weekday[article['weekday']].append(article)

    # ç»Ÿè®¡æ¯ä¸ªæ˜ŸæœŸçš„å¹³å‡æ•°æ®
    weekday_stats = {}
    for weekday, arts in by_weekday.items():
        weekday_name = ['å‘¨ä¸€', 'å‘¨äºŒ', 'å‘¨ä¸‰', 'å‘¨å››', 'å‘¨äº”', 'å‘¨å…­', 'å‘¨æ—¥'][weekday]
        weekday_stats[weekday_name] = {
            'count': len(arts),
            'avg_reads': statistics.mean([a['read_num'] for a in arts]),
            'avg_engagement': statistics.mean([a['engagement_rate'] for a in arts]),
            'total_reads': sum([a['read_num'] for a in arts])
        }

    # æŒ‰å°æ—¶åˆ†ç»„
    by_hour = defaultdict(list)
    for article in articles:
        if article['read_num'] > 0:
            by_hour[article['publish_hour']].append(article)

    # ç»Ÿè®¡æ¯ä¸ªå°æ—¶çš„å¹³å‡æ•°æ®
    hour_stats = {}
    for hour, arts in by_hour.items():
        hour_stats[hour] = {
            'count': len(arts),
            'avg_reads': statistics.mean([a['read_num'] for a in arts]),
            'avg_engagement': statistics.mean([a['engagement_rate'] for a in arts])
        }

    return {
        'by_weekday': weekday_stats,
        'by_hour': hour_stats
    }


def classify_title_type(title):
    """åˆ†ç±»æ ‡é¢˜ç±»å‹"""
    types = []

    # æ•°å­—å‹
    if re.search(r'\d+', title):
        types.append('æ•°å­—å‹')

    # å¯¹æ¯”å‹
    if any(word in title for word in ['vs', 'VS', 'å¯¹æ¯”', 'è¶…è¶Š', 'ç¡¬åˆš', 'è°æ›´å¼º', 'PK']):
        types.append('å¯¹æ¯”å‹')

    # åŠé€€å‹
    if any(word in title for word in ['åŠé€€', 'ä¸è¦', 'åˆ«å†', 'ä¸ç”¨', 'è¿‡æ—¶']):
        types.append('åŠé€€å‹')

    # å®æˆ˜å‹
    if any(word in title for word in ['å®æˆ˜', 'æ•™ç¨‹', 'æŠ€å·§', 'æ–¹æ³•', 'ç©æ³•', 'ç­–ç•¥', 'å…¨æµç¨‹', 'å®æµ‹', 'æµ‹è¯•']):
        types.append('å®æˆ˜å‹')

    # çˆ†æ–™å‹
    if any(word in title for word in ['æ›å…‰', 'æ­ç§˜', 'æ‹†è§£', 'æ‰’', 'æ·±åº¦', 'è§£æ']):
        types.append('çˆ†æ–™å‹')

    # é—®é¢˜å‹
    if 'ï¼Ÿ' in title or '?' in title:
        types.append('é—®é¢˜å‹')

    # æ–°é—»å‹
    if any(word in title for word in ['çªå‘', 'åˆš', 'å‘å¸ƒ', 'æ¨å‡º', 'ä¸Šçº¿', 'å¼€æº']):
        types.append('æ–°é—»å‹')

    # è§‚ç‚¹å‹
    if any(word in title for word in ['çœŸè§‰å¾—', 'æˆ‘è®¤ä¸º', 'å…¶å®', 'æ‰æ˜¯', 'å¯èƒ½æ˜¯']):
        types.append('è§‚ç‚¹å‹')

    # æ‚¬å¿µå‹
    if any(word in title for word in ['ç«Ÿç„¶', 'æ„å¤–', 'æƒ³ä¸åˆ°', 'å‡ºä¹æ„æ–™', 'å±…ç„¶']):
        types.append('æ‚¬å¿µå‹')

    return types if types else ['æ™®é€šå‹']


def analyze_title_types(articles):
    """åˆ†ææ ‡é¢˜ç±»å‹ä¸äº’åŠ¨ç‡çš„å…³ç³»"""
    type_stats = defaultdict(lambda: {
        'count': 0,
        'total_reads': 0,
        'total_engagement': 0,
        'articles': []
    })

    for article in articles:
        if article['read_num'] == 0:
            continue

        types = classify_title_type(article['title'])

        for title_type in types:
            type_stats[title_type]['count'] += 1
            type_stats[title_type]['total_reads'] += article['read_num']
            type_stats[title_type]['total_engagement'] += article['engagement_rate']
            type_stats[title_type]['articles'].append(article)

    # è®¡ç®—å¹³å‡å€¼
    results = {}
    for title_type, stats in type_stats.items():
        if stats['count'] > 0:
            results[title_type] = {
                'count': stats['count'],
                'avg_reads': stats['total_reads'] / stats['count'],
                'avg_engagement': stats['total_engagement'] / stats['count'],
                'total_reads': stats['total_reads']
            }

    return results


def analyze_content_length(articles):
    """åˆ†æå†…å®¹é•¿åº¦ä¸æ•°æ®çš„å…³ç³»"""
    # æŒ‰é•¿åº¦åˆ†æ®µ
    length_ranges = [
        (0, 1000, 'æçŸ­ (<1K)'),
        (1000, 3000, 'çŸ­ (1-3K)'),
        (3000, 5000, 'ä¸­ (3-5K)'),
        (5000, 8000, 'é•¿ (5-8K)'),
        (8000, 15000, 'å¾ˆé•¿ (8-15K)'),
        (15000, float('inf'), 'è¶…é•¿ (>15K)')
    ]

    range_stats = defaultdict(lambda: {
        'count': 0,
        'articles': []
    })

    for article in articles:
        if article['read_num'] == 0:
            continue

        length = article['content_length']
        for min_len, max_len, label in length_ranges:
            if min_len <= length < max_len:
                range_stats[label]['count'] += 1
                range_stats[label]['articles'].append(article)
                break

    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    results = {}
    for label, stats in range_stats.items():
        if stats['count'] > 0:
            articles = stats['articles']
            results[label] = {
                'count': stats['count'],
                'avg_reads': statistics.mean([a['read_num'] for a in articles]),
                'avg_engagement': statistics.mean([a['engagement_rate'] for a in articles]),
                'avg_length': statistics.mean([a['content_length'] for a in articles])
            }

    return results


def print_timeline_analysis(articles, days=30):
    """æ‰“å°å®Œæ•´çš„æ—¶é—´çº¿åˆ†ææŠ¥å‘Š"""
    print("=" * 80)
    print(f"ğŸ“… æœ€è¿‘ {days} å¤©æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()

    if not articles:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
        return

    # 1. æ›´æ–°é¢‘ç‡åˆ†æ
    print("=" * 80)
    print("ğŸ“Š 1. å†…å®¹æ›´æ–°é¢‘ç‡åˆ†æ")
    print("=" * 80)
    print()

    freq = analyze_update_frequency(articles)

    print(f"ğŸ“† ç»Ÿè®¡å‘¨æœŸ: {freq['total_days']} å¤©")
    print(f"ğŸ“ æ€»å‘å¸ƒæ–‡ç« : {freq['total_articles']} ç¯‡")
    print(f"ğŸ“… æœ‰å†…å®¹çš„å¤©æ•°: {freq['days_with_content']} å¤©")
    print(f"ğŸ“ˆ å‘å¸ƒé¢‘ç‡: {freq['publish_rate']:.1f}% (å¹³å‡æ¯ {100/freq['publish_rate']:.1f} å¤©å‘å¸ƒä¸€æ¬¡)" if freq['publish_rate'] > 0 else "")
    print()
    print(f"ğŸ“Š å‘å¸ƒèŠ‚å¥:")
    print(f"   â€¢ å¹³å‡æ¯å¤©: {freq['avg_per_day']:.2f} ç¯‡")
    print(f"   â€¢ æœ‰æ›´æ–°çš„æ—¥å­å¹³å‡: {freq['avg_per_active_day']:.2f} ç¯‡")
    print(f"   â€¢ å•æ—¥æœ€å¤š: {freq['max_per_day']} ç¯‡")
    print()

    # æ˜¾ç¤ºæœ€è¿‘çš„å‘å¸ƒæ—¥æœŸ
    recent_dates = sorted(freq['by_date'].keys(), reverse=True)[:10]
    print("ğŸ—“ï¸  æœ€è¿‘10å¤©çš„å‘å¸ƒæƒ…å†µ:")
    for date_str in recent_dates:
        date_obj = datetime.strptime(date_str, "%Y%m%d")
        articles_count = len(freq['by_date'][date_str])
        print(f"   â€¢ {date_obj.strftime('%Y-%m-%d')} ({date_obj.strftime('%A')}): {articles_count} ç¯‡")
    print()

    # 2. æœ€ä½³å‘å¸ƒæ—¶é—´åˆ†æ
    print("=" * 80)
    print("â° 2. æœ€ä½³å‘å¸ƒæ—¶é—´åˆ†æ")
    print("=" * 80)
    print()

    time_stats = analyze_best_publish_time(articles)

    print("ğŸ“… æŒ‰æ˜ŸæœŸå‡ åˆ†æ:")
    print()

    # æŒ‰å¹³å‡é˜…è¯»æ•°æ’åº
    weekday_sorted = sorted(
        time_stats['by_weekday'].items(),
        key=lambda x: x[1]['avg_reads'],
        reverse=True
    )

    for weekday_name, stats in weekday_sorted:
        print(f"   {weekday_name}:")
        print(f"      â€¢ å‘å¸ƒæ¬¡æ•°: {stats['count']} ç¯‡")
        print(f"      â€¢ å¹³å‡é˜…è¯»: {stats['avg_reads']:,.0f}")
        print(f"      â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement']:.2f}%")
        print()

    best_weekday = weekday_sorted[0][0]
    print(f"ğŸ† æœ€ä½³å‘å¸ƒæ—¥: {best_weekday} (å¹³å‡é˜…è¯» {weekday_sorted[0][1]['avg_reads']:,.0f})")
    print()

    print("ğŸ• æŒ‰å‘å¸ƒæ—¶é—´æ®µåˆ†æ:")
    print()

    # æŒ‰å°æ—¶åˆ†ç»„å¹¶æ’åº
    hour_sorted = sorted(
        time_stats['by_hour'].items(),
        key=lambda x: x[1]['avg_reads'],
        reverse=True
    )

    for hour, stats in hour_sorted[:5]:
        print(f"   {hour:02d}:00 - {hour:02d}:59")
        print(f"      â€¢ å‘å¸ƒæ¬¡æ•°: {stats['count']} ç¯‡")
        print(f"      â€¢ å¹³å‡é˜…è¯»: {stats['avg_reads']:,.0f}")
        print(f"      â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement']:.2f}%")
        print()

    # 3. æ ‡é¢˜ç±»å‹åˆ†æ
    print("=" * 80)
    print("ğŸ“ 3. æ ‡é¢˜ç±»å‹ä¸äº’åŠ¨ç‡åˆ†æ")
    print("=" * 80)
    print()

    title_stats = analyze_title_types(articles)

    # æŒ‰å¹³å‡äº’åŠ¨ç‡æ’åº
    title_sorted = sorted(
        title_stats.items(),
        key=lambda x: x[1]['avg_engagement'],
        reverse=True
    )

    print("ğŸ† å„ç±»å‹æ ‡é¢˜æ•ˆæœæ’åï¼ˆæŒ‰äº’åŠ¨ç‡ï¼‰:")
    print()

    for i, (title_type, stats) in enumerate(title_sorted, 1):
        print(f"{i}. ã€{title_type}ã€‘")
        print(f"   â€¢ ä½¿ç”¨æ¬¡æ•°: {stats['count']} ç¯‡")
        print(f"   â€¢ å¹³å‡é˜…è¯»: {stats['avg_reads']:,.0f}")
        print(f"   â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement']:.2f}%")
        print(f"   â€¢ æ€»é˜…è¯»æ•°: {stats['total_reads']:,}")
        print()

    # 4. å†…å®¹é•¿åº¦åˆ†æ
    print("=" * 80)
    print("ğŸ“ 4. å†…å®¹é•¿åº¦ä¸æ•°æ®å…³ç³»åˆ†æ")
    print("=" * 80)
    print()

    length_stats = analyze_content_length(articles)

    # æŒ‰å¹³å‡é˜…è¯»æ•°æ’åº
    length_sorted = sorted(
        length_stats.items(),
        key=lambda x: x[1]['avg_reads'],
        reverse=True
    )

    print("ğŸ“Š ä¸åŒé•¿åº¦æ–‡ç« çš„è¡¨ç°:")
    print()

    for label, stats in length_sorted:
        print(f"   ã€{label}ã€‘")
        print(f"      â€¢ æ–‡ç« æ•°: {stats['count']} ç¯‡")
        print(f"      â€¢ å¹³å‡å­—æ•°: {stats['avg_length']:,.0f}")
        print(f"      â€¢ å¹³å‡é˜…è¯»: {stats['avg_reads']:,.0f}")
        print(f"      â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement']:.2f}%")
        print()

    # 5. ç»¼åˆå»ºè®®
    print("=" * 80)
    print("ğŸ’¡ 5. æ•°æ®æ´å¯Ÿä¸å»ºè®®")
    print("=" * 80)
    print()

    print("âœ… å‘å¸ƒç­–ç•¥å»ºè®®:")
    print(f"   â€¢ æœ€ä½³å‘å¸ƒæ—¥: {best_weekday}")
    if hour_sorted:
        best_hour = hour_sorted[0][0]
        print(f"   â€¢ æœ€ä½³å‘å¸ƒæ—¶é—´: {best_hour:02d}:00 - {best_hour:02d}:59")
    print(f"   â€¢ å»ºè®®å‘å¸ƒé¢‘ç‡: {freq['avg_per_day']:.1f} ç¯‡/å¤©")
    print()

    print("âœ… æ ‡é¢˜ç­–ç•¥å»ºè®®:")
    if title_sorted:
        top3_types = [t[0] for t in title_sorted[:3]]
        print(f"   â€¢ ä¼˜å…ˆä½¿ç”¨: {', '.join(top3_types)}")
    print()

    print("âœ… å†…å®¹é•¿åº¦å»ºè®®:")
    if length_sorted:
        best_length = length_sorted[0][0]
        print(f"   â€¢ æœ€ä½³é•¿åº¦åŒºé—´: {best_length}")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨åŠ è½½æ–‡ç« æ•°æ®...")
    articles = load_article_data_with_time()

    if not articles:
        print("âŒ æœªæ‰¾åˆ°æ–‡ç« æ•°æ®")
        return

    # ç­›é€‰æœ€è¿‘30å¤©
    recent_articles = filter_recent_days(articles, days=30)

    if not recent_articles:
        print("âŒ æœªæ‰¾åˆ°æœ€è¿‘30å¤©çš„æ•°æ®")
        return

    print(f"âœ… å·²åŠ è½½ {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­æœ€è¿‘30å¤©æœ‰ {len(recent_articles)} ç¯‡")
    print()

    # åªåˆ†ææœ‰äº’åŠ¨æ•°æ®çš„
    articles_with_stats = [a for a in recent_articles if a['read_num'] > 0]

    if not articles_with_stats:
        print("âŒ æœªæ‰¾åˆ°æœ‰äº’åŠ¨æ•°æ®çš„æ–‡ç« ")
        return

    # æ‰“å°åˆ†ææŠ¥å‘Š
    print_timeline_analysis(articles_with_stats, days=30)


if __name__ == "__main__":
    main()
