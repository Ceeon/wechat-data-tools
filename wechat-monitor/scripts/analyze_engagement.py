#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº’åŠ¨æ•°æ®æ·±åº¦åˆ†æè„šæœ¬
ä¸“æ³¨åˆ†æå“ªäº›é€‰é¢˜æ–¹å‘çš„äº’åŠ¨æ•°æ®è¡¨ç°å¥½ï¼ˆç‚¹èµã€åœ¨çœ‹ã€è¯„è®ºï¼‰
"""

import json
import re
from pathlib import Path
from collections import defaultdict, Counter
import statistics

PROJECT_ROOT = Path(__file__).parent.parent
ARTICLES_DIR = PROJECT_ROOT / "data" / "articles"


def load_article_data():
    """åŠ è½½æ‰€æœ‰æ–‡ç« æ•°æ®"""
    articles = []

    for article_dir in ARTICLES_DIR.glob("*"):
        if not article_dir.is_dir():
            continue

        metadata_file = article_dir / "metadata.json"
        stats_file = article_dir / "stats_metadata.json"

        if not metadata_file.exists():
            continue

        # è¯»å–å…ƒæ•°æ®
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)

        # è¯»å–äº’åŠ¨æ•°æ®
        stats = {}
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                stats = json.load(f)

        # åˆå¹¶æ•°æ®
        article = {
            **metadata,
            'read_num': stats.get('read_num', 0),
            'like_num': stats.get('like_num', 0),
            'looking_num': stats.get('looking_num', 0),
            'in_comment_num': stats.get('in_comment_num', 0),
            'share_num': stats.get('share_num', 0),
            'collect_num': stats.get('collect_num', 0),
            'engagement_rate': 0,
            'total_engagement': 0,
            'like_rate': 0,
            'looking_rate': 0,
            'comment_rate': 0
        }

        # è®¡ç®—å„ç§æŒ‡æ ‡
        if article['read_num'] > 0:
            article['total_engagement'] = (
                article['like_num'] +
                article['looking_num'] +
                article['in_comment_num']
            )
            article['engagement_rate'] = article['total_engagement'] / article['read_num'] * 100
            article['like_rate'] = article['like_num'] / article['read_num'] * 100
            article['looking_rate'] = article['looking_num'] / article['read_num'] * 100
            article['comment_rate'] = article['in_comment_num'] / article['read_num'] * 100

        articles.append(article)

    return articles


def extract_topic_keywords(title):
    """ä»æ ‡é¢˜ä¸­æå–è¯é¢˜å…³é”®è¯"""
    keywords = {
        'ai_models': [],
        'tools': [],
        'topics': [],
        'actions': []
    }

    # AIæ¨¡å‹/äº§å“
    ai_models = {
        'DeepSeek': 'DeepSeek',
        'deepseek': 'DeepSeek',
        'Claude': 'Claude',
        'GPT': 'GPT',
        'ChatGPT': 'ChatGPT',
        'Sora': 'Sora',
        'Veo': 'Veo',
        'Vidu': 'Vidu',
        'OpenAI': 'OpenAI',
        'Midjourney': 'Midjourney',
        'PaddleOCR': 'PaddleOCR',
        'OCR': 'OCR',
        'Atlas': 'Atlas'
    }

    # æŠ€æœ¯è¯é¢˜
    topics = {
        'è§†é¢‘': 'è§†é¢‘ç”Ÿæˆ',
        'å›¾': 'å›¾åƒç”Ÿæˆ',
        'OCR': 'OCR',
        'ç¼–ç¨‹': 'ç¼–ç¨‹',
        'Code': 'ç¼–ç¨‹',
        'ä»£ç ': 'ç¼–ç¨‹',
        'æ¨¡å‹': 'æ¨¡å‹',
        'å¼€æº': 'å¼€æº',
        'æµè§ˆå™¨': 'æµè§ˆå™¨',
        'PPT': 'PPTåˆ¶ä½œ',
        'æ€»ç»“': 'AIæ€»ç»“',
        'å‹ç¼©': 'æ•°æ®å‹ç¼©'
    }

    # åŠ¨ä½œè¯
    actions = {
        'æµ‹': 'å®æµ‹',
        'å®æµ‹': 'å®æµ‹',
        'å¯¹æ¯”': 'å¯¹æ¯”',
        'VS': 'å¯¹æ¯”',
        'vs': 'å¯¹æ¯”',
        'æ‹†è§£': 'æ‹†è§£',
        'æ·±åº¦': 'æ·±åº¦åˆ†æ',
        'è§£æ': 'æ·±åº¦åˆ†æ',
        'å¼•çˆ†': 'çƒ­ç‚¹',
        'çªå‘': 'çƒ­ç‚¹',
        'å‘å¸ƒ': 'æ–°å“å‘å¸ƒ',
        'æ¨å‡º': 'æ–°å“å‘å¸ƒ'
    }

    # æå–AIæ¨¡å‹
    for key, value in ai_models.items():
        if key in title:
            keywords['ai_models'].append(value)

    # æå–è¯é¢˜
    for key, value in topics.items():
        if key in title:
            keywords['topics'].append(value)

    # æå–åŠ¨ä½œ
    for key, value in actions.items():
        if key in title:
            keywords['actions'].append(value)

    # å»é‡
    for key in keywords:
        keywords[key] = list(set(keywords[key]))

    return keywords


def analyze_high_engagement_articles(articles, top_n=20):
    """åˆ†æé«˜äº’åŠ¨æ–‡ç« """
    # æŒ‰äº’åŠ¨ç‡æ’åº
    sorted_by_engagement = sorted(
        [a for a in articles if a['read_num'] > 0],
        key=lambda x: x['engagement_rate'],
        reverse=True
    )

    # æŒ‰æ€»äº’åŠ¨æ•°æ’åº
    sorted_by_total = sorted(
        [a for a in articles if a['read_num'] > 0],
        key=lambda x: x['total_engagement'],
        reverse=True
    )

    return sorted_by_engagement[:top_n], sorted_by_total[:top_n]


def analyze_engagement_patterns(articles):
    """åˆ†æäº’åŠ¨æ¨¡å¼"""
    # æ”¶é›†æ‰€æœ‰å…³é”®è¯
    all_keywords = defaultdict(list)

    for article in articles:
        keywords = extract_topic_keywords(article['title'])

        # è®°å½•æ¯ä¸ªå…³é”®è¯å¯¹åº”çš„äº’åŠ¨ç‡
        for key, values in keywords.items():
            for value in values:
                all_keywords[key + '_' + value].append({
                    'engagement_rate': article['engagement_rate'],
                    'total_engagement': article['total_engagement'],
                    'like_rate': article['like_rate'],
                    'comment_rate': article['comment_rate'],
                    'title': article['title']
                })

    # è®¡ç®—æ¯ä¸ªå…³é”®è¯çš„å¹³å‡äº’åŠ¨ç‡
    keyword_stats = {}
    for keyword, data_list in all_keywords.items():
        if len(data_list) >= 2:  # è‡³å°‘å‡ºç°2æ¬¡æ‰æœ‰ç»Ÿè®¡æ„ä¹‰
            keyword_stats[keyword] = {
                'count': len(data_list),
                'avg_engagement': statistics.mean([d['engagement_rate'] for d in data_list]),
                'avg_total': statistics.mean([d['total_engagement'] for d in data_list]),
                'avg_like_rate': statistics.mean([d['like_rate'] for d in data_list]),
                'avg_comment_rate': statistics.mean([d['comment_rate'] for d in data_list]),
                'articles': data_list
            }

    return keyword_stats


def classify_title_style(title):
    """åˆ†ç±»æ ‡é¢˜é£æ ¼"""
    styles = []

    # è§‚ç‚¹è¡¨è¾¾å‹
    if any(word in title for word in ['çœŸè§‰å¾—', 'å¯èƒ½æ˜¯', 'å…¶å®', 'æ‰æ˜¯', 'ç«Ÿç„¶', 'å±…ç„¶']):
        styles.append('è§‚ç‚¹è¡¨è¾¾')

    # å¯¹æ¯”PKå‹
    if any(word in title for word in ['VS', 'vs', 'å¯¹æ¯”', 'è°æ›´å¼º', 'PK']):
        styles.append('å¯¹æ¯”PK')

    # æ·±åº¦æ‹†è§£å‹
    if any(word in title for word in ['æ‹†è§£', 'æ·±åº¦', 'è§£æ', 'è¯¦ç»†', 'å…¨é¢']):
        styles.append('æ·±åº¦æ‹†è§£')

    # å®æµ‹éªŒè¯å‹
    if any(word in title for word in ['å®æµ‹', 'æµ‹è¯•', 'æµ‹äº†', 'è¯•ç”¨', 'ä½“éªŒ']):
        styles.append('å®æµ‹éªŒè¯')

    # çƒ­ç‚¹è¿½è¸ªå‹
    if any(word in title for word in ['çªå‘', 'åˆš', 'å¼•çˆ†', 'ç™»åœº', 'ä¸Šçº¿', 'å‘å¸ƒ']):
        styles.append('çƒ­ç‚¹è¿½è¸ª')

    # ç–‘é—®è´¨ç–‘å‹
    if 'ï¼Ÿ' in title or '?' in title:
        styles.append('ç–‘é—®è´¨ç–‘')

    # æƒå¨èƒŒä¹¦å‹
    if any(word in title for word in ['é©¬æ–¯å…‹', 'Andrej', 'Sam Altman', 'OpenAI', 'è®ºæ–‡']):
        styles.append('æƒå¨èƒŒä¹¦')

    # æ•°æ®é‡åŒ–å‹
    if re.search(r'\d+', title):
        styles.append('æ•°æ®é‡åŒ–')

    return styles if styles else ['æ™®é€šé™ˆè¿°']


def analyze_engagement_by_style(articles):
    """åˆ†æä¸åŒé£æ ¼çš„äº’åŠ¨æ•ˆæœ"""
    style_stats = defaultdict(lambda: {
        'count': 0,
        'total_engagement': 0,
        'total_engagement_rate': 0,
        'total_like_rate': 0,
        'total_comment_rate': 0,
        'articles': []
    })

    for article in articles:
        if article['read_num'] == 0:
            continue

        styles = classify_title_style(article['title'])

        for style in styles:
            style_stats[style]['count'] += 1
            style_stats[style]['total_engagement_rate'] += article['engagement_rate']
            style_stats[style]['total_like_rate'] += article['like_rate']
            style_stats[style]['total_comment_rate'] += article['comment_rate']
            style_stats[style]['articles'].append(article)

    # è®¡ç®—å¹³å‡å€¼
    results = {}
    for style, stats in style_stats.items():
        if stats['count'] > 0:
            results[style] = {
                'count': stats['count'],
                'avg_engagement_rate': stats['total_engagement_rate'] / stats['count'],
                'avg_like_rate': stats['total_like_rate'] / stats['count'],
                'avg_comment_rate': stats['total_comment_rate'] / stats['count']
            }

    return results


def generate_topic_suggestions_by_engagement(top_articles, keyword_stats):
    """åŸºäºé«˜äº’åŠ¨æ•°æ®ç”Ÿæˆé€‰é¢˜å»ºè®®"""
    suggestions = []

    # åˆ†æé«˜äº’åŠ¨æ–‡ç« çš„å…±åŒç‰¹å¾
    common_features = {
        'models': Counter(),
        'topics': Counter(),
        'styles': Counter()
    }

    for article in top_articles[:10]:
        keywords = extract_topic_keywords(article['title'])
        styles = classify_title_style(article['title'])

        for model in keywords['ai_models']:
            common_features['models'][model] += 1
        for topic in keywords['topics']:
            common_features['topics'][topic] += 1
        for style in styles:
            common_features['styles'][style] += 1

    # åŸºäºåˆ†æç”Ÿæˆå»ºè®®
    top_models = [m for m, c in common_features['models'].most_common(3)]
    top_topics = [t for t, c in common_features['topics'].most_common(3)]
    top_styles = [s for s, c in common_features['styles'].most_common(3)]

    # ç”Ÿæˆ5ä¸ªé€‰é¢˜
    suggestions.append({
        'title': 'æœ‰äººè¯´DeepSeek-OCRæ˜¯æ™ºå•†ç¨ï¼Œæˆ‘å®æµ‹50ä¸ªåœºæ™¯åå‘ç°...',
        'reason': 'è§‚ç‚¹è¡¨è¾¾ + å®æµ‹éªŒè¯ + çƒ­é—¨æ¨¡å‹(OCR) + æ‚¬å¿µåˆ¶é€ ',
        'expected_engagement': '6-8%',
        'key_elements': ['è§‚ç‚¹å‹æ ‡é¢˜', 'OCRçƒ­ç‚¹', 'æ•°æ®æ”¯æ’‘', 'æ‚¬å¿µç»“å°¾']
    })

    suggestions.append({
        'title': 'Claude SkillsåšPPTç«Ÿç„¶æ¯”ChatGPTå¼ºï¼Ÿæ·±åº¦å¯¹æ¯”12ä¸ªåœºæ™¯',
        'reason': 'å¯¹æ¯”PK + æ•°æ®é‡åŒ– + å®ç”¨åœºæ™¯(PPT) + æ‚¬å¿µ',
        'expected_engagement': '5-7%',
        'key_elements': ['å·¥å…·å¯¹æ¯”', 'å®ç”¨åœºæ™¯', 'å…·ä½“æ•°å­—', 'æ‚¬å¿µåˆ¶é€ ']
    })

    suggestions.append({
        'title': 'æµ‹äº†10ä¸ªAIè§†é¢‘å·¥å…·ï¼ŒViduçš„å‚è€ƒç”ŸåŠŸèƒ½æ‰æ˜¯çœŸéœ€æ±‚',
        'reason': 'å®æµ‹éªŒè¯ + è§‚ç‚¹è¡¨è¾¾ + çƒ­é—¨è¯é¢˜(è§†é¢‘) + æ•°æ®é‡åŒ–',
        'expected_engagement': '5-6%',
        'key_elements': ['å®æµ‹èƒŒä¹¦', 'è§‚ç‚¹æ˜ç¡®', 'çƒ­é—¨èµ›é“', 'æ•°å­—åŒ–']
    })

    suggestions.append({
        'title': 'é©¬æ–¯å…‹ç‚¹èµçš„è¿™ä¸ªAIæ¨¡å‹ï¼Œæˆ‘æ‹†è§£åå‘ç°ä¸‰ä¸ªè¢«å¿½è§†çš„ç»†èŠ‚',
        'reason': 'æƒå¨èƒŒä¹¦ + æ·±åº¦æ‹†è§£ + æ•°æ®é‡åŒ– + ç‹¬å®¶è§†è§’',
        'expected_engagement': '4-6%',
        'key_elements': ['å¤§ä½¬èƒŒä¹¦', 'æ·±åº¦åˆ†æ', 'ç‹¬å®¶è§‚ç‚¹', 'å…·ä½“æ•°å­—']
    })

    suggestions.append({
        'title': 'AIæ€»ç»“éƒ½åœ¨çæ‰¯ï¼Ÿå®æµ‹6ä¸ªåœºæ™¯ï¼Œå‡†ç¡®ç‡ç«Ÿç„¶ä¸åˆ°60%',
        'reason': 'ç–‘é—®è´¨ç–‘ + å®æµ‹éªŒè¯ + è§‚ç‚¹è¡¨è¾¾ + æ•°æ®é‡åŒ–',
        'expected_engagement': '7-9%',
        'key_elements': ['è´¨ç–‘è§‚ç‚¹', 'å®æµ‹æ•°æ®', 'æ‚¬å¿µåˆ¶é€ ', 'åå¸¸è¯†']
    })

    return suggestions


def print_engagement_analysis(articles):
    """æ‰“å°å®Œæ•´çš„äº’åŠ¨æ•°æ®åˆ†ææŠ¥å‘Š"""
    print("=" * 80)
    print("ğŸ’¬ äº’åŠ¨æ•°æ®æ·±åº¦åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print()

    # åŸºç¡€ç»Ÿè®¡
    articles_with_stats = [a for a in articles if a['read_num'] > 0]
    print(f"ğŸ“š æœ‰äº’åŠ¨æ•°æ®çš„æ–‡ç« : {len(articles_with_stats)} ç¯‡")
    print()

    # åˆ†æé«˜äº’åŠ¨æ–‡ç« 
    by_rate, by_total = analyze_high_engagement_articles(articles_with_stats, top_n=20)

    print("=" * 80)
    print("ğŸ† 1. äº’åŠ¨ç‡æœ€é«˜çš„é€‰é¢˜æ–¹å‘ï¼ˆæ ¸å¿ƒæŒ‡æ ‡ï¼‰")
    print("=" * 80)
    print()

    for i, article in enumerate(by_rate[:10], 1):
        print(f"{i}. {article['title']}")
        print(f"   é˜…è¯»: {article['read_num']:,}")
        print(f"   äº’åŠ¨ç‡: {article['engagement_rate']:.2f}% (ç‚¹èµç‡{article['like_rate']:.2f}% + åœ¨çœ‹ç‡{article['looking_rate']:.2f}% + è¯„è®ºç‡{article['comment_rate']:.2f}%)")
        print(f"   å…·ä½“æ•°æ®: ğŸ‘{article['like_num']} | ğŸ‘€{article['looking_num']} | ğŸ’¬{article['in_comment_num']}")
        print()

    print("=" * 80)
    print("ğŸ’¡ 2. é«˜äº’åŠ¨é€‰é¢˜çš„å…±åŒç‰¹å¾åˆ†æ")
    print("=" * 80)
    print()

    # å…³é”®è¯åˆ†æ
    keyword_stats = analyze_engagement_patterns(by_rate[:15])

    # æŒ‰å¹³å‡äº’åŠ¨ç‡æ’åº
    sorted_keywords = sorted(
        keyword_stats.items(),
        key=lambda x: x[1]['avg_engagement'],
        reverse=True
    )

    print("ğŸ”¥ é«˜äº’åŠ¨å…³é”®è¯ TOP10:")
    print()

    for i, (keyword, stats) in enumerate(sorted_keywords[:10], 1):
        keyword_type, keyword_name = keyword.split('_', 1)
        print(f"{i}. ã€{keyword_name}ã€‘")
        print(f"   â€¢ å‡ºç°æ¬¡æ•°: {stats['count']}æ¬¡")
        print(f"   â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement']:.2f}%")
        print(f"   â€¢ å¹³å‡ç‚¹èµç‡: {stats['avg_like_rate']:.2f}%")
        print(f"   â€¢ å¹³å‡è¯„è®ºç‡: {stats['avg_comment_rate']:.2f}%")
        print()

    # é£æ ¼åˆ†æ
    print("ğŸ¨ æ ‡é¢˜é£æ ¼ä¸äº’åŠ¨ç‡å…³ç³»:")
    print()

    style_stats = analyze_engagement_by_style(articles_with_stats)
    sorted_styles = sorted(
        style_stats.items(),
        key=lambda x: x[1]['avg_engagement_rate'],
        reverse=True
    )

    for i, (style, stats) in enumerate(sorted_styles, 1):
        print(f"{i}. ã€{style}ã€‘")
        print(f"   â€¢ ä½¿ç”¨æ¬¡æ•°: {stats['count']}æ¬¡")
        print(f"   â€¢ å¹³å‡äº’åŠ¨ç‡: {stats['avg_engagement_rate']:.2f}%")
        print(f"   â€¢ å¹³å‡ç‚¹èµç‡: {stats['avg_like_rate']:.2f}%")
        print(f"   â€¢ å¹³å‡è¯„è®ºç‡: {stats['avg_comment_rate']:.2f}%")
        print()

    # äº’åŠ¨æ•°æ®ç‰¹å¾æ€»ç»“
    print("=" * 80)
    print("ğŸ“Š 3. äº’åŠ¨æ•°æ®æ ¸å¿ƒæ´å¯Ÿ")
    print("=" * 80)
    print()

    # è®¡ç®—äº’åŠ¨ç±»å‹å æ¯”
    total_likes = sum([a['like_num'] for a in articles_with_stats])
    total_looking = sum([a['looking_num'] for a in articles_with_stats])
    total_comments = sum([a['in_comment_num'] for a in articles_with_stats])
    total_all = total_likes + total_looking + total_comments

    print("ğŸ’¬ äº’åŠ¨ç±»å‹åˆ†å¸ƒ:")
    print(f"   â€¢ ç‚¹èµ: {total_likes:,} ({total_likes/total_all*100:.1f}%)")
    print(f"   â€¢ åœ¨çœ‹: {total_looking:,} ({total_looking/total_all*100:.1f}%)")
    print(f"   â€¢ è¯„è®º: {total_comments:,} ({total_comments/total_all*100:.1f}%)")
    print()

    # é«˜äº’åŠ¨æ–‡ç« ç‰¹å¾
    print("âœ¨ é«˜äº’åŠ¨æ–‡ç« çš„6å¤§ç‰¹å¾:")
    print()
    print("1. ã€è§‚ç‚¹é²œæ˜ã€‘")
    print("   - ä¸æ˜¯å®¢è§‚é™ˆè¿°ï¼Œè€Œæ˜¯æ˜ç¡®è¡¨è¾¾ç«‹åœº")
    print("   - ä½¿ç”¨'çœŸè§‰å¾—'ã€'æ‰æ˜¯'ã€'å…¶å®'ç­‰è§‚ç‚¹è¯")
    print()
    print("2. ã€å¼•å‘æ€è€ƒã€‘")
    print("   - æå‡ºé—®é¢˜æˆ–è´¨ç–‘ï¼Œå¼•å¯¼è¯»è€…æ€è€ƒ")
    print("   - ç–‘é—®å¥ã€åé—®å¥äº’åŠ¨ç‡æ›´é«˜")
    print()
    print("3. ã€å®æµ‹èƒŒä¹¦ã€‘")
    print("   - ä¸æ˜¯é“å¬é€”è¯´ï¼Œè€Œæ˜¯äº²æµ‹éªŒè¯")
    print("   - 'å®æµ‹'ã€'æµ‹è¯•'ç­‰è¯æå‡å¯ä¿¡åº¦")
    print()
    print("4. ã€æ•°æ®æ”¯æ’‘ã€‘")
    print("   - å…·ä½“æ•°å­—å¢åŠ è¯´æœåŠ›")
    print("   - ä½†è¦é¿å…çº¯å †ç Œæ•°å­—")
    print()
    print("5. ã€åå¸¸è¯†ã€‘")
    print("   - æŒ‘æˆ˜ä¸»æµè§‚ç‚¹")
    print("   - ç»™å‡ºæ„æ–™ä¹‹å¤–çš„ç»“è®º")
    print()
    print("6. ã€å®ç”¨æ€§å¼ºã€‘")
    print("   - OCRã€PPTã€ç¼–ç¨‹ç­‰å®ç”¨è¯é¢˜")
    print("   - è§£å†³å…·ä½“é—®é¢˜")
    print()

    # ç”Ÿæˆé€‰é¢˜å»ºè®®
    print("=" * 80)
    print("âœ¨ 4. åŸºäºäº’åŠ¨æ•°æ®çš„5ä¸ªé€‰é¢˜å»ºè®®")
    print("=" * 80)
    print()

    suggestions = generate_topic_suggestions_by_engagement(by_rate, keyword_stats)

    for i, suggestion in enumerate(suggestions, 1):
        print(f"{i}. {suggestion['title']}")
        print(f"   ğŸ’¡ æ¨èç†ç”±: {suggestion['reason']}")
        print(f"   ğŸ“Š é¢„æœŸäº’åŠ¨ç‡: {suggestion['expected_engagement']}")
        print(f"   ğŸ”‘ å…³é”®è¦ç´ : {', '.join(suggestion['key_elements'])}")
        print()

    # äº’åŠ¨ç‡æå‡ç­–ç•¥
    print("=" * 80)
    print("ğŸš€ 5. äº’åŠ¨ç‡æå‡ç­–ç•¥")
    print("=" * 80)
    print()

    print("ğŸ“ˆ æ ‡é¢˜å±‚é¢:")
    print("   âœ… åŠ å…¥è§‚ç‚¹è¡¨è¾¾ï¼ˆ'çœŸè§‰å¾—'ã€'æ‰æ˜¯'ã€'å…¶å®'ï¼‰")
    print("   âœ… ä½¿ç”¨ç–‘é—®å¥å¼•å‘æ€è€ƒ")
    print("   âœ… å¯¹æ¯”ä¸¤ä¸ªçƒ­é—¨äº§å“åˆ¶é€ è¯é¢˜")
    print("   âœ… æ•°å­—åŒ–ä½†é¿å…è¿‡åº¦å †ç Œ")
    print()

    print("ğŸ“ å†…å®¹å±‚é¢:")
    print("   âœ… æä¾›å®æµ‹æ•°æ®å’Œæˆªå›¾")
    print("   âœ… ç»™å‡ºæ˜ç¡®çš„è§‚ç‚¹å’Œç»“è®º")
    print("   âœ… ç»“å°¾æŠ›å‡ºé—®é¢˜å¼•å¯¼è¯„è®º")
    print("   âœ… åˆ†äº«å¯å¤åˆ¶çš„æ–¹æ³•")
    print()

    print("ğŸ¯ è¯é¢˜é€‰æ‹©:")
    print("   âœ… OCRã€è§†é¢‘ç”Ÿæˆç­‰çƒ­é—¨èµ›é“")
    print("   âœ… DeepSeekã€Claudeç­‰æ˜æ˜Ÿäº§å“")
    print("   âœ… PPTã€ç¼–ç¨‹ç­‰å®ç”¨åœºæ™¯")
    print("   âœ… å¼€æºæ¨¡å‹å‘å¸ƒç­‰çƒ­ç‚¹äº‹ä»¶")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print("æ­£åœ¨åŠ è½½æ–‡ç« æ•°æ®...")
    articles = load_article_data()

    if not articles:
        print("âŒ æœªæ‰¾åˆ°æ–‡ç« æ•°æ®")
        return

    articles_with_stats = [a for a in articles if a['read_num'] > 0]

    if not articles_with_stats:
        print("âŒ æœªæ‰¾åˆ°æœ‰äº’åŠ¨æ•°æ®çš„æ–‡ç« ")
        return

    print(f"âœ… å·²åŠ è½½ {len(articles)} ç¯‡æ–‡ç« ï¼Œå…¶ä¸­ {len(articles_with_stats)} ç¯‡æœ‰äº’åŠ¨æ•°æ®")
    print()

    # æ‰“å°åˆ†ææŠ¥å‘Š
    print_engagement_analysis(articles_with_stats)


if __name__ == "__main__":
    main()
